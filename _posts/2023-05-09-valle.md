---
layout: post
title: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers 논문 리뷰
subtitle: VALL-E 논문 리뷰
categories: 
    - paper review
tags: 
    - SpeechSynthesis
    - TTS
use_math: true
---
최근 Stable diffusion, ChatGPT 등과 같은 Generative AI가 쏟아져 나오면서 음성 합성 분야에서도 이러한 방법론을 도입하려는 시도가 몇몇 보이는 것 같다. 그런 의미에서.. 오늘 리뷰 할 논문은 TTS에 Neural codec과 Language modeling 방법을 적절히 합쳐서 만든 speech synthesis를 위한 generative model이라고 볼 수 있을 것 같다. :)

### Introduction
![Overview](/assets/images/posts/2023-05-11-valle/overview.png)
#### Motivation
- 기존 TTS의 경우 학습을 위해 고품질의 studio data가 필요
- Internet에 있는 방대한 데이터는 품질이 좋지 않아 성능 저하를 유발
- 기존 TTS는 large data를 활용하지 못하기 때문에 generalization 성능이 떨어짐 (unseen에 취약)
- text synthesis 분야에서의 성공을 speech synthesis 분야로 가져옴

#### Goal
- 방대한 Internet data와 LM 학습 기법을 활용해 zero-shot TTS를 만드는 것

#### Contribution
- VALL-E를 제안
    - first TTS framework with strong in-context learning capabilities
    - prompt 기반의 zero-shot TTS
    - simple scaling up semi-supervised data가 TTS에 대해 과소평가 되었다(?)는 것을 보여줌
    - 같은 input text에 대해 다양한 감정과 녹음 환경을 표현 가능
    - zero-shot 성능도 좋음

### Background: Speech Quantization
![codec](/assets/images/posts/2023-05-11-valle/encodec.png)
- audio는 보통 16-bit integer sequence로 저장되기 때문에 generative model은 timestep당 65,536개의 확률을 출력해야 함, 오디오 길이도 매우 김
- 이를 해결하기 위한 몇 가지 기법 등장
    - mu-law quantization
        - 확률값의 개수를 65,536 → 256개로 줄였지만 여전히 오디오 길이는 매우 긺
- vq-wav2vec, HuBERT같은 VQ 기반의 speech representation 모델
    - content 복원이 가능, 속도 빠름
    - poor reconstruction quality, speaker identity 버려짐
- Neural codec
    - 풍부한 speaker information, acoustic information
    - off-the-shelf codec decoder가 존재하여 vocoder 따로 필요 X
    - time-steps의 길이를 줄일 수 있음 (mu-law의 단점 보완)
    - 논문에서는 Encodec이라는 neural codec을 사용

### VALL-E
#### Problem Formulation: Regarding TTS as Conditional Codec Language Modeling
- Dataset
    $$ \mathcal{D}=\{x_i, y_i\} $$
    - $y_i$ : audio sample
    - $\mathbf{x}=\{x_0,x_1,\ldots,x_L\}$ : corresponding phoneme transcription
- Pre-trained neural codec model
    $$Encodec(\mathbf{y})=\mathbf{C}^{T \times 8} \\
    Decodec(\mathbf{C}) \approx \hat{\mathbf{y}}$$
    - $\mathbf{C}$ : two-dimensional acoustic code matrix
    - $T$ : downsampled utterance length
    - $\mathbf{c}_{t,:}$ : frame t에 대한 8개의 codes를 표현 (row)
    - $\mathbf{c}_{:,j}$ : j-th codebook의 code sequence (column)
- Objective
    $$
    \max p(\mathbf{C}|\mathbf{x},\tilde{\mathbf{C}})
    $$
    
    - $\mathbf{x}$ : phoneme sequence
    - $\tilde{\mathbf{C}}^{T' \times 8}$ : acoustic prompt matrix ($\mathbf{C}$와 동일한 encoder로 만들어짐)

#### Training: Conditional Codec Language Modeling
- Autoregressive (AR) decoder와 Non-autoregressive (NAR) decoder를 혼합하여 모델링

![Valle](/assets/images/posts/2023-05-11-valle/valle.png)

- first quantizer의 discrete tokens $\mathbf{c_{:,1}}$를 얻기 위한 AR 모델 (길이에 유연)

$$
p(\mathbf{c}_{:,1}|\mathbf{x},\tilde{\mathbf{C}}_{:,1};\theta_{AR})=\prod_{t=0}^{T}p(\mathbf{c}_{<t,1},\tilde{\mathbf{c}}_{:,1},\mathbf{x};\theta_{AR})
$$

- second quantizer부터 last quantizer의 discrete tokens $\mathbf{c}_{:,j\in[2,8]}$를 얻기 위한 NAR 모델

$$
p(\mathbf{C}_{:,2:8}|\mathbf{x},\tilde{\mathbf{C}};\theta_{NAR})=\prod_{j=2}^{8}p(c_{:,j}|\mathbf{C}_{:,<j},\mathbf{x},\tilde{\mathbf{C}};\theta_{NAR})
$$

- Overall model
    
    $$
    p(\mathbf{C}|\mathbf{x},\tilde{\mathbf{C}};\theta)=p(\mathbf{c}_{:,1}|\tilde{\mathbf{C}}_{:,1},\mathbf{X};\theta_{AR})\prod_{j=2}^{8}p(\mathbf{c}_{:,\mathbf{j}}|\mathbf{c}_{:,<j},\mathbf{x},\tilde{\mathbf{C}};|\theta_{NAR})
    $$
    

#### Inference: In-Context Learning via Prompting

- Notation
    - $c_{:, 1}$ : first quantizer code for input text
    - $\tilde{c}_{:, 1}$ : acoustic prompt
    - $X$ : input text
    - $\tilde{X}$ : a transciption for $\tilde{C}$
- VALL-E (원하는 문장을 생성)
    - Input : $(\tilde{x}, x, \tilde{c}_{:,1})$
    - Output : $c_{:, 1}$
- VALL-E-continual (처음 3초에 이어서 생성)
    - Input : $(\tilde{x}, \tilde{c}_{3s, 1})$
    - Output : $\tilde{c}_{:, 1}$

### Experiment
#### Experiment Setup
- Dataset
    - LibriLight
        - 60K hours
        - 7000 speakers
        - hybrid DNN-HMM ASR model을 사용하여 labelling 진행
    - LibriSpeech
        - 960 hours
        - hybrid DNN-HMM ASR model 학습을 위해 사용
- Model
    - transformer
        - 12 layers
        - 16 attention heads
        - embedding dimension of 1024
        - feed-forward layer dimension of 4096
        - dropout of 0.1
- Baseline
    - YourTTS (SOTA zero-shot TTS model)
- Automatic metrics
    - speaker similarity
        - WavLM-TDNN이라는 모델을 사용하여 similarity score 계산 (높을수록 좋음)
- Human evaluation
    - comparative mean opinion score (CMOS)
        - 12명의 native speakers
    - similarity mean opinion score (SMOS)
        - 6명의 native speakers

### Reference
* ["Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers" paper](https://arxiv.org/pdf/2301.02111)