---
layout: post
title: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers 논문 리뷰
subtitle: VALL-E 논문 리뷰
categories: 
    - paper review
tags: 
    - SpeechSynthesis
    - TTS
use_math: true
---
최근 Stable diffusion, ChatGPT, LaMa 등을 보면 우리는 그야말로 Generative AI 붐인 시대에 살고있는 것 같습니다. 이러한 세간의 흐름에 맞춰 음성 합성 분야에서도 large-scale data와 Generative model을 활용한 다양한 논문들이 쏟아지고 있으며 오늘은 그 중 하나인 "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"라는 논문을 리뷰하고자 합니다. 

해당 논문에서 다루는 Task는 Zero-shot TTS task로 보이며, Neural Audio Codec과 Language model을 활용하여 codec의 codes를 예측하는 새로운 TTS paradigm을 제시한 점이 눈여겨 볼 부분인 것 같습니다.

정리는 개인적으로 중요하다고 생각되는 부분을 위주로 하였으며 자세한 내용은 논문을 참고하시길 바랍니다 :pray:

### Introduction
---
![Overview](/assets/images/posts/2023-05-11-valle/overview.png)
#### Motivation
- 기존 TTS의 경우 학습을 위해 고품질의 studio data가 필요
- Internet에 있는 방대한 데이터는 품질이 좋지 않아 성능 저하를 유발
- 기존 TTS는 large data를 활용하지 못하기 때문에 generalization 성능이 떨어짐 (unseen에 취약)
- text synthesis 분야에서의 성공을 speech synthesis 분야로 가져옴

#### Goal
- 방대한 Internet data와 LM 학습 기법을 활용해 zero-shot TTS를 만드는 것

#### Contribution
- VALL-E를 제안
    - first TTS framework with strong in-context learning capabilities
    - prompt 기반의 zero-shot TTS
    - simple scaling up semi-supervised data가 TTS에 대해 과소평가 되었다(?)는 것을 보여줌
    - 같은 input text에 대해 다양한 감정과 녹음 환경을 표현 가능
    - zero-shot 성능도 좋음

### Background: Speech Quantization
---
![codec](/assets/images/posts/2023-05-11-valle/encodec.png)
- audio는 2가지 측면에서 generative model이 모델링하기 매우 어려운 데이터
    - frame rate : 보통 16-bit integer sequence로 저장되기 때문에 timestep 당 65,536개의 값을 가질 수 있음
    - time steps : 보통 1초에 적게는 16,000에서 많게는 48000개의 sample을 가짐
- 이를 해결하기 위한 몇 가지 기법 등장
    - mu-law quantization
        - 확률값의 개수를 65,536 → 256개로 줄였지만 여전히 오디오 길이는 매우 긺
- vq-wav2vec, HuBERT같은 VQ 기반의 speech representation 모델
    - content 복원이 가능, 속도 빠름
    - poor reconstruction quality, speaker identity 버려짐
- Neural codec
    - 풍부한 speaker information, acoustic information
    - off-the-shelf codec decoder가 존재하여 vocoder 따로 필요 X
    - time-steps의 길이를 줄일 수 있음 (mu-law의 단점 보완)
    - 논문에서는 Encodec이라는 neural codec을 사용

### VALL-E
---
#### Problem Formulation: Regarding TTS as Conditional Codec Language Modeling
- Dataset

    $$\mathcal{D}=\{x_i, y_i\}$$

    - $y_i$ : audio sample
    - $\mathbf{x}=\{x_0,x_1,\ldots,x_L\}$ : corresponding phoneme transcription
- Pre-trained neural codec model

    $$Encodec(\mathbf{y})=\mathbf{C}^{T \times 8} \\
    Decodec(\mathbf{C}) \approx \hat{\mathbf{y}}$$

    - $\mathbf{C}$ : two-dimensional acoustic code matrix
    - $T$ : downsampled utterance length
    - $\mathbf{c}_{t,:}$ : frame t에 대한 8개의 codes를 표현 (row)
    - $\mathbf{c}_{:,j}$ : j-th codebook의 code sequence (column)
- Objective
    $$\max p(\mathbf{C}|\mathbf{x},\tilde{\mathbf{C}})$$
    
    - $\mathbf{x}$ : phoneme sequence
    - $\tilde{\mathbf{C}}^{T' \times 8}$ : acoustic prompt matrix ($\mathbf{C}$와 동일한 encoder로 만들어짐)

#### Training: Conditional Codec Language Modeling
- Autoregressive (AR) decoder\와\ Non-autoregressive (NAR) decoder를 혼합하여 모델링

![Valle](/assets/images/posts/2023-05-11-valle/valle.png)

- first quantizer의 discrete tokens $\mathbf{c_{:,1}}$를 얻기 위한 AR 모델 (길이에 유연)

    $$
    p(\mathbf{c}_{:,1}|\mathbf{x},\tilde{\mathbf{C}}_{:,1};\theta_{AR})=\prod_{t=0}^{T}p(\mathbf{c}_{<t,1},\tilde{\mathbf{c}}_{:,1},\mathbf{x};\theta_{AR})
    $$
    
    - 눈여겨 볼 점은 \\(\tilde{\mathbf{c}}_{:,1}\\) 와 \\(\mathbf{c}_{:,1}\\)사이엔 &lt;sep&gt; 토큰이 들어가지 않음

- second quantizer부터 last quantizer의 discrete tokens $\mathbf{c}_{:,j\in[2,8]}$를 얻기 위한 NAR 모델

    $$
    p(\mathbf{C}_{:,2:8}|\mathbf{x},\tilde{\mathbf{C}};\theta_{NAR})=\prod_{j=2}^{8}p(c_{:,j}|\mathbf{C}_{:,<j},\mathbf{x},\tilde{\mathbf{C}};\theta_{NAR})
    $$

- Overall model
    
    $$
    p(\mathbf{C}|\mathbf{x},\tilde{\mathbf{C}};\theta)=p(\mathbf{c}_{:,1}|\tilde{\mathbf{C}}_{:,1},\mathbf{X};\theta_{AR})\prod_{j=2}^{8}p(\mathbf{c}_{:,\mathbf{j}}|\mathbf{c}_{:,<j},\mathbf{x},\tilde{\mathbf{C}};|\theta_{NAR})
    $$
    

#### Inference: In-Context Learning via Prompting

- Notation
    - $c_{:, 1}$ : first quantizer code for input text
    - $\tilde{c}_{:, 1}$ : acoustic prompt
    - $X$ : input text
    - $\tilde{X}$ : a transciption for $\tilde{C}$
- VALL-E (원하는 문장을 생성)
    - Input : $(\tilde{x}, x, \tilde{c}_{:,1})$
    - Output : $c_{:, 1}$
- VALL-E-continual (처음 3초에 이어서 생성)
    - Input : $(\tilde{x}, \tilde{c}_{3s, 1})$
    - Output : $\tilde{c}_{:, 1}$

### Experiment
---
#### Experiment Setup
- Dataset
    - LibriLight
        - 60K hours
        - 7000 speakers
        - hybrid DNN-HMM ASR model을 사용하여 labelling 진행
    - LibriSpeech
        - 960 hours
        - hybrid DNN-HMM ASR model 학습을 위해 사용
- Model
    - transformer
        - 12 layers
        - 16 attention heads
        - embedding dimension of 1024
        - feed-forward layer dimension of 4096
        - dropout of 0.1
- Baseline
    - YourTTS (SOTA zero-shot TTS model)
- Automatic metrics
    - speaker similarity
        - WavLM-TDNN이라는 모델을 사용하여 similarity score 계산 (높을수록 좋음)
- Human evaluation
    - comparative mean opinion score (CMOS)
        - 12명의 native speakers
    - similarity mean opinion score (SMOS)
        - 6명의 native speakers

### Reference
---
* ["Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers" paper](https://arxiv.org/pdf/2301.02111)